{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9512cc-c6e2-4b5b-8a8a-df19d6f1bd60",
   "metadata": {},
   "source": [
    "# Database Performance Optimization: Practical Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9916833c-c253-457b-8e48-f5c5b8276386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb4f0168-976a-4360-9536-da12fc7faad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a table with 1000000 records.\n"
     ]
    }
   ],
   "source": [
    "# Database Performance Optimization: Practical Examples\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "\n",
    "# Create a sample database\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a large table for demonstration\n",
    "cursor.execute('''\n",
    "CREATE TABLE orders (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    customer_id INTEGER,\n",
    "    order_date TEXT,\n",
    "    total_amount FLOAT\n",
    ")\n",
    "''')\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "num_records = 1000000\n",
    "customer_ids = np.random.randint(1, 10001, num_records)\n",
    "order_dates = pd.date_range(start='2020-01-01', end='2023-12-31', periods=num_records).strftime('%Y-%m-%d')\n",
    "total_amounts = np.random.uniform(10, 1000, num_records)\n",
    "\n",
    "# Insert data into the table\n",
    "cursor.executemany(\n",
    "    'INSERT INTO orders (customer_id, order_date, total_amount) VALUES (?, ?, ?)',\n",
    "    zip(customer_ids, order_dates, total_amounts)\n",
    ")\n",
    "conn.commit()\n",
    "\n",
    "print(f\"Created a table with {num_records} records.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc87621-f082-4635-b79d-f7c45db34535",
   "metadata": {},
   "source": [
    "## Example 1: Demonstrating the impact of indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae64a34a-017a-4934-bf2b-057fc0213761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1: Impact of Indexing\n",
      "Query time without index: 0.0261 seconds\n"
     ]
    }
   ],
   "source": [
    "# Function to measure query execution time\n",
    "def measure_query_time(query):\n",
    "    start_time = time.time()\n",
    "    cursor.execute(query)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "print(\"\\nExample 1: Impact of Indexing\")\n",
    "\n",
    "# Query without index\n",
    "query_no_index = \"SELECT * FROM orders WHERE customer_id = 5000\"\n",
    "time_no_index = measure_query_time(query_no_index)\n",
    "print(f\"Query time without index: {time_no_index:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be227f58-0739-4667-9ee4-9c1079e02c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query time with index: 0.0000 seconds\n",
      "Performance improvement: 99.91%\n"
     ]
    }
   ],
   "source": [
    "# Create an index on customer_id\n",
    "cursor.execute(\"CREATE INDEX idx_customer_id ON orders (customer_id)\")\n",
    "conn.commit()\n",
    "\n",
    "# Query with index\n",
    "time_with_index = measure_query_time(query_no_index)\n",
    "print(f\"Query time with index: {time_with_index:.4f} seconds\")\n",
    "print(f\"Performance improvement: {(time_no_index - time_with_index) / time_no_index * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e6869-cea9-4bf9-9fe1-9b2f6eeb5a1e",
   "metadata": {},
   "source": [
    "## Example 2: Query Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3626cbce-016c-40e4-af27-ceb0c3ce36cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2: Query Optimization\n",
      "Inefficient query time: 0.6339 seconds\n",
      "Efficient query time: 0.5819 seconds\n",
      "Performance improvement: 8.22%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample 2: Query Optimization\")\n",
    "\n",
    "# Inefficient query\n",
    "inefficient_query = \"\"\"\n",
    "SELECT customer_id, COUNT(*) as order_count\n",
    "FROM orders\n",
    "WHERE total_amount > 500\n",
    "GROUP BY customer_id\n",
    "HAVING COUNT(*) > 10\n",
    "ORDER BY order_count DESC\n",
    "\"\"\"\n",
    "\n",
    "# Efficient query\n",
    "efficient_query = \"\"\"\n",
    "SELECT customer_id, COUNT(*) as order_count\n",
    "FROM orders\n",
    "WHERE total_amount > 500\n",
    "GROUP BY customer_id\n",
    "HAVING order_count > 10\n",
    "ORDER BY order_count DESC\n",
    "\"\"\"\n",
    "\n",
    "time_inefficient = measure_query_time(inefficient_query)\n",
    "time_efficient = measure_query_time(efficient_query)\n",
    "\n",
    "print(f\"Inefficient query time: {time_inefficient:.4f} seconds\")\n",
    "print(f\"Efficient query time: {time_efficient:.4f} seconds\")\n",
    "print(f\"Performance improvement: {(time_inefficient - time_efficient) / time_inefficient * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab5004d-8fd3-4e2d-ad85-e179dcba74a2",
   "metadata": {},
   "source": [
    "## Example 3: Caching Strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cd2d461-9006-40c5-bb0a-1abed3a51d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 3: Caching Strategy\n",
      "Time without cache (1000 calls): 0.0007 seconds\n",
      "Time with cache (1000 calls): 0.0003 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample 3: Caching Strategy\")\n",
    "\n",
    "import functools\n",
    "\n",
    "# Simple cache implementation\n",
    "def simple_cache(func):\n",
    "    cache = {}\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        key = str(args) + str(kwargs)\n",
    "        if key not in cache:\n",
    "            cache[key] = func(*args, **kwargs)\n",
    "        return cache[key]\n",
    "    return wrapper\n",
    "\n",
    "# Function to get total orders for a customer\n",
    "@simple_cache\n",
    "def get_total_orders(customer_id):\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM orders WHERE customer_id = ?\", (customer_id,))\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "# Measure time without cache\n",
    "start_time = time.time()\n",
    "for _ in range(1000):\n",
    "    get_total_orders(5000)\n",
    "end_time = time.time()\n",
    "print(f\"Time without cache (1000 calls): {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "# Measure time with cache\n",
    "start_time = time.time()\n",
    "for _ in range(1000):\n",
    "    get_total_orders(5000)\n",
    "end_time = time.time()\n",
    "print(f\"Time with cache (1000 calls): {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6e308-6c03-4489-8fef-ddace589fd4b",
   "metadata": {},
   "source": [
    "## Example 4: Batch Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db9d8dc1-26f8-456c-9f79-3920001c92d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 4: Batch Processing\n",
      "Time for individual inserts (100 records): 0.0005 seconds\n",
      "Time for batch insert (10000 records): 0.0346 seconds\n",
      "Batch insert is approximately 1.47 times faster per record\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample 4: Batch Processing\")\n",
    "\n",
    "# Function to insert records one by one\n",
    "def insert_individual(records):\n",
    "    start_time = time.time()\n",
    "    for record in records:\n",
    "        cursor.execute(\"INSERT INTO orders (customer_id, order_date, total_amount) VALUES (?, ?, ?)\", record)\n",
    "    conn.commit()\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Function to insert records in batch\n",
    "def insert_batch(records):\n",
    "    start_time = time.time()\n",
    "    cursor.executemany(\"INSERT INTO orders (customer_id, order_date, total_amount) VALUES (?, ?, ?)\", records)\n",
    "    conn.commit()\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Generate sample records\n",
    "sample_records = [\n",
    "    (np.random.randint(1, 10001), \n",
    "     (pd.Timestamp('2023-01-01') + pd.Timedelta(days=i)).strftime('%Y-%m-%d'), \n",
    "     np.random.uniform(10, 1000))\n",
    "    for i in range(10000)\n",
    "]\n",
    "\n",
    "individual_time = insert_individual(sample_records[:100])  # Using only 100 records for individual insert to save time\n",
    "batch_time = insert_batch(sample_records)\n",
    "\n",
    "print(f\"Time for individual inserts (100 records): {individual_time:.4f} seconds\")\n",
    "print(f\"Time for batch insert (10000 records): {batch_time:.4f} seconds\")\n",
    "print(f\"Batch insert is approximately {individual_time / (batch_time / 100):.2f} times faster per record\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f99bf-21f2-4b84-b764-2118334b1eba",
   "metadata": {},
   "source": [
    "## Example 5: Advanced Query Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb6df4aa-4b97-47f3-b9f9-eddc49cf0523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 5: Advanced Query Optimization\n",
      "Subquery execution time: 0.0544 seconds\n",
      "JOIN query execution time: 0.2078 seconds\n",
      "Performance difference: 73.84%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample 5: Advanced Query Optimization\")\n",
    "\n",
    "# Create a new table for this example\n",
    "cursor.execute('''\n",
    "CREATE TABLE sales (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    product_id INTEGER,\n",
    "    sale_date TEXT,\n",
    "    quantity INTEGER,\n",
    "    price FLOAT\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insert sample data\n",
    "sample_sales = [\n",
    "    (np.random.randint(1, 101),  # product_id\n",
    "     (pd.Timestamp('2023-01-01') + pd.Timedelta(days=i % 365)).strftime('%Y-%m-%d'),  # sale_date\n",
    "     np.random.randint(1, 11),  # quantity\n",
    "     np.random.uniform(10, 1000))  # price\n",
    "    for i in range(100000)\n",
    "]\n",
    "\n",
    "cursor.executemany(\n",
    "    'INSERT INTO sales (product_id, sale_date, quantity, price) VALUES (?, ?, ?, ?)',\n",
    "    sample_sales\n",
    ")\n",
    "conn.commit()\n",
    "\n",
    "# Subquery vs. JOIN\n",
    "subquery = \"\"\"\n",
    "SELECT o.id, o.customer_id, o.total_amount,\n",
    "    (SELECT SUM(s.quantity * s.price)\n",
    "     FROM sales s\n",
    "     WHERE s.sale_date = o.order_date) as daily_sales\n",
    "FROM orders o\n",
    "WHERE o.order_date BETWEEN '2023-01-01' AND '2023-01-31'\n",
    "\"\"\"\n",
    "\n",
    "join_query = \"\"\"\n",
    "SELECT o.id, o.customer_id, o.total_amount, SUM(s.quantity * s.price) as daily_sales\n",
    "FROM orders o\n",
    "LEFT JOIN sales s ON s.sale_date = o.order_date\n",
    "WHERE o.order_date BETWEEN '2023-01-01' AND '2023-01-31'\n",
    "GROUP BY o.id, o.customer_id, o.total_amount, o.order_date\n",
    "\"\"\"\n",
    "\n",
    "subquery_time = measure_query_time(subquery)\n",
    "join_time = measure_query_time(join_query)\n",
    "\n",
    "print(f\"Subquery execution time: {subquery_time:.4f} seconds\")\n",
    "print(f\"JOIN query execution time: {join_time:.4f} seconds\")\n",
    "print(f\"Performance difference: {abs(subquery_time - join_time) / max(subquery_time, join_time) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ce68c8-8e40-49c6-b891-732218570d1d",
   "metadata": {},
   "source": [
    "## Example 6: Index Impact on Writes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9a259bf-98a8-4f35-b257-b6af6f6d6b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 6: Index Impact on Writes and Reads (Further Optimized)\n",
      "Insert time for orders_no_index (1,000,000 records): 14.8512 seconds\n",
      "Insert time for orders_with_indexes (1,000,000 records): 15.0276 seconds\n",
      "\n",
      "Read Performance Comparison:\n",
      "\n",
      "Range Query:\n",
      "  Without index: 0.0569 seconds\n",
      "  With index: 0.0018 seconds\n",
      "  Performance improvement: 96.81%\n",
      "\n",
      "Group By Query:\n",
      "  Without index: 0.2475 seconds\n",
      "  With index: 0.0356 seconds\n",
      "  Performance improvement: 85.62%\n",
      "\n",
      "Join Query:\n",
      "  Without index: 31.0959 seconds\n",
      "  With index: 71.3658 seconds\n",
      "  Performance improvement: -129.50%\n",
      "\n",
      "Note: The impact of indexes on read performance can vary based on the specific query and data distribution.\n",
      "In general, indexes provide the most benefit for queries that filter or join on indexed columns,\n",
      "especially when dealing with large datasets and selective queries.\n"
     ]
    }
   ],
   "source": [
    "# Example 6: Index Impact on Writes and Reads (Further Optimized)\n",
    "print(\"\\nExample 6: Index Impact on Writes and Reads (Further Optimized)\")\n",
    "\n",
    "import sqlite3\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a new connection to ensure a fresh start\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def measure_insert_time(num_records, table_name):\n",
    "    start_time = time.time()\n",
    "    records = [\n",
    "        (np.random.randint(1, 10001),\n",
    "         (pd.Timestamp('2023-01-01') + pd.Timedelta(days=i % 365)).strftime('%Y-%m-%d'),\n",
    "         np.random.uniform(10, 1000),\n",
    "         f\"Product-{np.random.randint(1, 1001)}\",\n",
    "         np.random.choice(['A', 'B', 'C', 'D', 'E']))\n",
    "        for i in range(num_records)\n",
    "    ]\n",
    "    cursor.executemany(\n",
    "        f'INSERT INTO {table_name} (customer_id, order_date, total_amount, product_name, category) VALUES (?, ?, ?, ?, ?)',\n",
    "        records\n",
    "    )\n",
    "    conn.commit()\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Create tables and insert data\n",
    "for table_name in ['orders_no_index', 'orders_with_indexes']:\n",
    "    cursor.execute(f'''\n",
    "    CREATE TABLE {table_name} (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        customer_id INTEGER,\n",
    "        order_date TEXT,\n",
    "        total_amount FLOAT,\n",
    "        product_name TEXT,\n",
    "        category TEXT\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    insert_time = measure_insert_time(1000000, table_name)\n",
    "    print(f\"Insert time for {table_name} (1,000,000 records): {insert_time:.4f} seconds\")\n",
    "\n",
    "# Create indexes after data insertion\n",
    "cursor.execute(\"CREATE INDEX idx_customer_id ON orders_with_indexes (customer_id)\")\n",
    "cursor.execute(\"CREATE INDEX idx_order_date ON orders_with_indexes (order_date)\")\n",
    "cursor.execute(\"CREATE INDEX idx_product_name ON orders_with_indexes (product_name)\")\n",
    "cursor.execute(\"CREATE INDEX idx_category ON orders_with_indexes (category)\")\n",
    "conn.commit()\n",
    "\n",
    "def measure_read_time(table_name, query):\n",
    "    start_time = time.time()\n",
    "    cursor.execute(query.format(table_name=table_name))\n",
    "    cursor.fetchall()\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Define multiple query types to test\n",
    "queries = {\n",
    "    \"Range Query\": \"\"\"\n",
    "    SELECT COUNT(*) \n",
    "    FROM {table_name}\n",
    "    WHERE order_date BETWEEN '2023-03-01' AND '2023-03-31'\n",
    "    \"\"\",\n",
    "    \"Group By Query\": \"\"\"\n",
    "    SELECT category, COUNT(*) as count\n",
    "    FROM {table_name}\n",
    "    GROUP BY category\n",
    "    \"\"\",\n",
    "    \"Join Query\": \"\"\"\n",
    "    SELECT o.category, COUNT(DISTINCT o.customer_id) as unique_customers\n",
    "    FROM {table_name} o\n",
    "    JOIN (SELECT customer_id FROM {table_name} WHERE total_amount > 500) high_value\n",
    "    ON o.customer_id = high_value.customer_id\n",
    "    GROUP BY o.category\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print(\"\\nRead Performance Comparison:\")\n",
    "for query_name, query in queries.items():\n",
    "    read_no_index = measure_read_time('orders_no_index', query)\n",
    "    read_with_index = measure_read_time('orders_with_indexes', query)\n",
    "    \n",
    "    improvement = (read_no_index - read_with_index) / read_no_index * 100\n",
    "    print(f\"\\n{query_name}:\")\n",
    "    print(f\"  Without index: {read_no_index:.4f} seconds\")\n",
    "    print(f\"  With index: {read_with_index:.4f} seconds\")\n",
    "    print(f\"  Performance improvement: {improvement:.2f}%\")\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "print(\"\\nNote: The impact of indexes on read performance can vary based on the specific query and data distribution.\")\n",
    "print(\"In general, indexes provide the most benefit for queries that filter or join on indexed columns,\")\n",
    "print(\"especially when dealing with large datasets and selective queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bef8f4-f2b1-4685-8abe-153506c6be89",
   "metadata": {},
   "source": [
    "# Example 7: Simulating Database Maintenance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8795d926-95c3-41a0-bf69-770e50ef397a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 7: Simulating Database Maintenance\n",
      "Inserting initial data...\n",
      "Initial query time: 0.0122 seconds\n",
      "Simulating fragmentation...\n",
      "Query time after fragmentation: 0.0113 seconds\n",
      "Performing maintenance operation...\n",
      "Query time after maintenance: 0.0118 seconds\n",
      "\n",
      "Fragmentation impact: -7.12% slower\n",
      "Maintenance improvement: -4.65% faster\n",
      "\n",
      "Note: This simulation uses an in-memory SQLite database, which may not show\n",
      "significant fragmentation effects. In real-world scenarios with disk-based\n",
      "databases, the impact of fragmentation and benefits of maintenance operations\n",
      "like rebuilding indexes or reorganizing data can be much more pronounced.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample 7: Simulating Database Maintenance\")\n",
    "\n",
    "import sqlite3\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a new connection to ensure a fresh start\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a table for this example\n",
    "cursor.execute('''\n",
    "CREATE TABLE orders (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    customer_id INTEGER,\n",
    "    order_date TEXT,\n",
    "    total_amount FLOAT\n",
    ")\n",
    "''')\n",
    "\n",
    "# Function to insert records\n",
    "def insert_records(num_records):\n",
    "    records = [\n",
    "        (np.random.randint(1, 10001),\n",
    "         (pd.Timestamp('2023-01-01') + pd.Timedelta(days=i % 365)).strftime('%Y-%m-%d'),\n",
    "         np.random.uniform(10, 1000))\n",
    "        for i in range(num_records)\n",
    "    ]\n",
    "    cursor.executemany(\n",
    "        'INSERT INTO orders (customer_id, order_date, total_amount) VALUES (?, ?, ?)',\n",
    "        records\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "# Function to measure query performance\n",
    "def measure_query_time(query):\n",
    "    start_time = time.time()\n",
    "    cursor.execute(query)\n",
    "    cursor.fetchall()\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Insert initial data\n",
    "print(\"Inserting initial data...\")\n",
    "insert_records(1000000)\n",
    "\n",
    "# Create an index\n",
    "cursor.execute(\"CREATE INDEX idx_customer_id ON orders (customer_id)\")\n",
    "conn.commit()\n",
    "\n",
    "# Measure initial query performance\n",
    "initial_query_time = measure_query_time(\"SELECT * FROM orders WHERE customer_id BETWEEN 5000 AND 5100\")\n",
    "print(f\"Initial query time: {initial_query_time:.4f} seconds\")\n",
    "\n",
    "# Simulate fragmentation by deleting and inserting records\n",
    "print(\"Simulating fragmentation...\")\n",
    "cursor.execute(\"DELETE FROM orders WHERE id % 3 = 0\")\n",
    "insert_records(300000)  # Insert new records to fill gaps and create fragmentation\n",
    "\n",
    "# Measure query performance after fragmentation\n",
    "fragmented_query_time = measure_query_time(\"SELECT * FROM orders WHERE customer_id BETWEEN 5000 AND 5100\")\n",
    "print(f\"Query time after fragmentation: {fragmented_query_time:.4f} seconds\")\n",
    "\n",
    "# Simulate maintenance operation (VACUUM)\n",
    "print(\"Performing maintenance operation...\")\n",
    "cursor.execute(\"VACUUM\")\n",
    "conn.commit()\n",
    "\n",
    "# Measure query performance after maintenance\n",
    "maintenance_query_time = measure_query_time(\"SELECT * FROM orders WHERE customer_id BETWEEN 5000 AND 5100\")\n",
    "print(f\"Query time after maintenance: {maintenance_query_time:.4f} seconds\")\n",
    "\n",
    "# Calculate and print performance changes\n",
    "frag_impact = (fragmented_query_time - initial_query_time) / initial_query_time * 100\n",
    "maintenance_improvement = (fragmented_query_time - maintenance_query_time) / fragmented_query_time * 100\n",
    "\n",
    "print(f\"\\nFragmentation impact: {frag_impact:.2f}% slower\")\n",
    "print(f\"Maintenance improvement: {maintenance_improvement:.2f}% faster\")\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "print(\"\\nNote: This simulation uses an in-memory SQLite database, which may not show\")\n",
    "print(\"significant fragmentation effects. In real-world scenarios with disk-based\")\n",
    "print(\"databases, the impact of fragmentation and benefits of maintenance operations\")\n",
    "print(\"like rebuilding indexes or reorganizing data can be much more pronounced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94da0d-673b-4a4a-bc9a-8194b8f07760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
